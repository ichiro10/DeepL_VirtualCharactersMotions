# DeepL_VirtualCharactersMotions
Deep Learning application for style control of virtual characters motions


Type of offer: Masters/Research Masters
Location: Rennes
Research topic: Deep Learning, animation, virtual reality
Project: VIRTUS-INRIA

# # # Subject:

In recent years, we have seen a significant development in creating digital doubles and
characters who closely resemble real humans. Creating believable behaviour and
animation for these virtual humans has remained challenging, especially when applied to
photorealistic characters . In this research project, we will be exploring the animation
qualities of photorealistic virtual humans. It has been shown that character movement can be
perceived as attractive, significantly affecting users’ behaviour towards them . Creating
appalling characters and their impact on the users will be the central subject of our study.
The objective of this research is to identify which aspects of the character animation affect
the user’s perception and interaction with the character (agent).

First, we will focus on representation of motion data in sparse components which can be combined to
produce movements of various styles (happy, depressed, calm, etc.) To do so we will train a Deep Learning model on large motion capture datasets.



![alt text](https://miro.medium.com/v2/resize:fit:640/1*qk1-U3Efd1YWNTp3L_mxCw.gif)

Figure 1: Motion style transfer: from depressed to proud


The design will be validated by conducting perceptual studies, particularly by harnessing the
power of VR which allows the measure of behavioural responses of the users to virtual
people, while our findings will be used to advance further research and provide design
guidelines for a wider spread application for entertainment, training and rehabilitation.





Literature:
[1] Metahumans, Unreal Engine: https://www.unrealengine.com/en-US/metahuman
[2] McDonnell, R., Breidt, M., & Bülthoff, H. H. (2012). Render me real?: investigating the effect of render style on the
perception of animated virtual humans. ACM Transactions on Graphics (TOG), 31(4), 91
[3] Zibrek, K., Niay, B., Olivier, A. H., Hoyet, L., Pettre, J., & McDonnell, R. (2020). The effect of gender and attractiveness of
motion on proximity in virtual reality. ACM Transactions on Applied Perception (TAP), 17(4), 1-15.
[4] I. Mason, S. Starke, H. Zhang, H. Bilen and T. Komura(2018), Few-shot Learning of Homogeneous Human Locomotion
Styles
[5] Daniel Holden and Jun Saito and Taku Komura (2016) A deep learning framework for character motion synthesis and
editing, ACM Transactions on Graphics (TOG)
[6] Kfir Aberman, Yijia Weng, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen.
Motion Style Transfer from Video to Animation, ACM Transactions on Graphics (SIGGRAPH 2020).
[7] Learned Motion Matching, DANIEL HOLDEN, Ubisoft La Forge, Ubisoft, OUSSAMA KANOUN, Ubisoft La Forge, Ubisoft,
MAKSYM PEREPICHKA, Concordia University, TIBERIU POPA, Concordia University, Canada
[8] A Deep Learning Framework for Character Motion Synthesis and Editing, Daniel Holden, Jun Saito, Taku Komura



